{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C1X-TeZp-co",
        "outputId": "581df7b2-9f27-496e-dc86-b8f3ad06980c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLaVA'...\n",
            "remote: Enumerating objects: 1151, done.\u001b[K\n",
            "remote: Counting objects: 100% (387/387), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 1151 (delta 317), reused 318 (delta 301), pack-reused 764\u001b[K\n",
            "Receiving objects: 100% (1151/1151), 11.87 MiB | 10.57 MiB/s, done.\n",
            "Resolving deltas: 100% (693/693), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/haotian-liu/LLaVA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCa_vLi5qFLW",
        "outputId": "19f3da94-cb84-4ea8-d45f-de0c4892b4ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.2.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mObtaining file:///content/LLaVA\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llava==1.0.1) (0.6.1)\n",
            "Collecting fastapi (from llava==1.0.1)\n",
            "  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/af/e0/ad7d3e6a7f550a5ddae10d475f54d982ae077a563ee508893aeb6fb380ee/fastapi-0.101.0-py3-none-any.whl.metadata\n",
            "  Downloading fastapi-0.101.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting gradio==3.35.2 (from llava==1.0.1)\n",
            "  Obtaining dependency information for gradio==3.35.2 from https://files.pythonhosted.org/packages/50/70/ed0ba0fb5c3b1cb2e481717ad190056a4c9a0ef2f296b871e10375b2ab83/gradio-3.35.2-py3-none-any.whl.metadata\n",
            "  Downloading gradio-3.35.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting markdown2[all] (from llava==1.0.1)\n",
            "  Obtaining dependency information for markdown2[all] from https://files.pythonhosted.org/packages/f1/98/61276a753f078dd2f3171c9a69fd3f451d220e806b2b1cdca41b8e368b0f/markdown2-2.4.10-py2.py3-none-any.whl.metadata\n",
            "  Downloading markdown2-2.4.10-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llava==1.0.1) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from llava==1.0.1) (2.27.1)\n",
            "Collecting sentencepiece (from llava==1.0.1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.12.1 (from llava==1.0.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from llava==1.0.1) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from llava==1.0.1) (0.15.2+cu118)\n",
            "Collecting uvicorn (from llava==1.0.1)\n",
            "  Obtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from llava==1.0.1) (0.15.8)\n",
            "Collecting shortuuid (from llava==1.0.1)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting httpx==0.24.0 (from llava==1.0.1)\n",
            "  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed==0.9.5 (from llava==1.0.1)\n",
            "  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting peft==0.4.0 (from llava==1.0.1)\n",
            "  Obtaining dependency information for peft==0.4.0 from https://files.pythonhosted.org/packages/88/a0/6e1c23293a922a9c9e9bd8d56a60cd78ecf531fdabe45ac975e142bfbe86/peft-0.4.0-py3-none-any.whl.metadata\n",
            "  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting transformers==4.31.0 (from llava==1.0.1)\n",
            "  Obtaining dependency information for transformers==4.31.0 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.21.0 (from llava==1.0.1)\n",
            "  Obtaining dependency information for accelerate==0.21.0 from https://files.pythonhosted.org/packages/70/f9/c381bcdd0c3829d723aa14eec8e75c6c377b4ca61ec68b8093d9f35fc7a7/accelerate-0.21.0-py3-none-any.whl.metadata\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting bitsandbytes==0.41.0 (from llava==1.0.1)\n",
            "  Obtaining dependency information for bitsandbytes==0.41.0 from https://files.pythonhosted.org/packages/b9/33/1cea2d1c909dd3e2b595f7b73c4417f3786c385a4b269a5c40c7699bb14b/bitsandbytes-0.41.0-py3-none-any.whl.metadata\n",
            "  Downloading bitsandbytes-0.41.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from llava==1.0.1) (1.2.2)\n",
            "Collecting einops-exts==0.0.4 (from llava==1.0.1)\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Collecting timm==0.6.13 (from llava==1.0.1)\n",
            "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client==0.2.9 (from llava==1.0.1)\n",
            "  Obtaining dependency information for gradio-client==0.2.9 from https://files.pythonhosted.org/packages/fc/be/50b4ba7b5ab067e31b67526f9cdbdd2241854af2f6b205f678c9da316ac9/gradio_client-0.2.9-py3-none-any.whl.metadata\n",
            "  Downloading gradio_client-0.2.9-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->llava==1.0.1) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->llava==1.0.1) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->llava==1.0.1) (6.0.1)\n",
            "Collecting hjson (from deepspeed==0.9.5->llava==1.0.1)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.9.5->llava==1.0.1) (1.11.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.9.5->llava==1.0.1) (9.0.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.9.5->llava==1.0.1) (1.10.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.9.5->llava==1.0.1) (4.65.0)\n",
            "Collecting aiofiles (from gradio==3.35.2->llava==1.0.1)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->llava==1.0.1) (3.8.5)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->llava==1.0.1) (4.2.2)\n",
            "Collecting ffmpy (from gradio==3.35.2->llava==1.0.1)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub>=0.14.0 (from gradio==3.35.2->llava==1.0.1)\n",
            "  Obtaining dependency information for huggingface-hub>=0.14.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->llava==1.0.1) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->llava==1.0.1) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->llava==1.0.1) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->llava==1.0.1) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.35.2->llava==1.0.1)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from gradio==3.35.2->llava==1.0.1)\n",
            "  Obtaining dependency information for orjson from https://files.pythonhosted.org/packages/a3/13/959dbe9e6cc77a0e50f617b79d49e21d0ac80a16838d4f2d2a172f76f363/orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->llava==1.0.1) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->llava==1.0.1) (9.4.0)\n",
            "Collecting pydub (from gradio==3.35.2->llava==1.0.1)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->llava==1.0.1) (2.14.0)\n",
            "Collecting python-multipart (from gradio==3.35.2->llava==1.0.1)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version (from gradio==3.35.2->llava==1.0.1)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting websockets>=10.0 (from gradio==3.35.2->llava==1.0.1)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.2.9->llava==1.0.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.2.9->llava==1.0.1) (4.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.24.0->llava==1.0.1) (2023.7.22)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx==0.24.0->llava==1.0.1)\n",
            "  Obtaining dependency information for httpcore<0.18.0,>=0.15.0 from https://files.pythonhosted.org/packages/94/2c/2bde7ff8dd2064395555220cbf7cba79991172bf5315a07eb3ac7688d9f1/httpcore-0.17.3-py3-none-any.whl.metadata\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx==0.24.0->llava==1.0.1) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.24.0->llava==1.0.1) (1.3.0)\n",
            "Collecting safetensors (from peft==0.4.0->llava==1.0.1)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->llava==1.0.1) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->llava==1.0.1) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->llava==1.0.1) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->llava==1.0.1) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->llava==1.0.1) (2022.10.31)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->llava==1.0.1) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->llava==1.0.1) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->llava==1.0.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->llava==1.0.1) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->llava==1.0.1) (16.0.6)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llava==1.0.1) (8.1.6)\n",
            "Collecting h11>=0.8 (from uvicorn->llava==1.0.1)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->llava==1.0.1)\n",
            "  Obtaining dependency information for starlette<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl.metadata\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting wavedrom (from markdown2[all]->llava==1.0.1)\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->llava==1.0.1) (1.26.16)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->llava==1.0.1) (2.0.12)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->llava==1.0.1) (3.1.32)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->llava==1.0.1) (1.29.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->llava==1.0.1) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb->llava==1.0.1) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->llava==1.0.1) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->llava==1.0.1) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->llava==1.0.1) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->llava==1.0.1) (3.20.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.35.2->llava==1.0.1) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.35.2->llava==1.0.1) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.35.2->llava==1.0.1) (0.12.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->llava==1.0.1) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb->llava==1.0.1) (4.0.10)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.0->llava==1.0.1) (3.7.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->llava==1.0.1) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->llava==1.0.1) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.35.2->llava==1.0.1)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is still looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.35.2->llava==1.0.1)\n",
            "  Obtaining dependency information for markdown-it-py[linkify]>=2.0.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.35.2->llava==1.0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.35.2->llava==1.0.1) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->llava==1.0.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->llava==1.0.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->llava==1.0.1) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->llava==1.0.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->llava==1.0.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->llava==1.0.1) (1.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.35.2->llava==1.0.1) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.35.2->llava==1.0.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.35.2->llava==1.0.1) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.35.2->llava==1.0.1) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.35.2->llava==1.0.1) (3.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->llava==1.0.1) (1.3.0)\n",
            "Collecting svgwrite (from wavedrom->markdown2[all]->llava==1.0.1)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx==0.24.0->llava==1.0.1) (1.1.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->llava==1.0.1) (5.0.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.35.2->llava==1.0.1) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->llava==1.0.1) (1.0.2)\n",
            "Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-3.35.2-py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-0.2.9-py3-none-any.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.101.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.4.10-py2.py3-none-any.whl (39 kB)\n",
            "Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llava, deepspeed, ffmpy, wavedrom\n",
            "  Building editable for llava (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llava: filename=llava-1.0.1-0.editable-py3-none-any.whl size=13802 sha256=cb147a68039445c2e13099b7f2461161b86cfb9a44e25cf2a58d5e1ca41c26fa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v9s3r9ud/wheels/04/eb/cc/8992f8302dd174d3e63566efe82795f070b535b03506b75ffb\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844532 sha256=7ecfe5e0c21bdce6ef6ac1fe16c988796a91506f5592b29d3c037724a1f0adf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=2cb0549ef52da43e720ca1ebecdbd2fc36c2915c419eeffecfd0c60cf43a981d\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=29934 sha256=aa83f89c47fc6122d3d7670d1dc3a529929c89a167486ceee4886aa47e472576\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
            "Successfully built llava deepspeed ffmpy wavedrom\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pydub, hjson, ffmpy, bitsandbytes, websockets, svgwrite, shortuuid, semantic-version, python-multipart, orjson, markdown2, markdown-it-py, h11, einops-exts, aiofiles, wavedrom, uvicorn, starlette, mdit-py-plugins, huggingface-hub, httpcore, transformers, httpx, fastapi, gradio-client, gradio, accelerate, timm, peft, deepspeed, llava\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "Successfully installed accelerate-0.21.0 aiofiles-23.1.0 bitsandbytes-0.41.0 deepspeed-0.9.5 einops-exts-0.0.4 fastapi-0.101.0 ffmpy-0.3.1 gradio-3.35.2 gradio-client-0.2.9 h11-0.14.0 hjson-3.1.0 httpcore-0.17.3 httpx-0.24.0 huggingface-hub-0.16.4 llava-1.0.1 markdown-it-py-2.2.0 markdown2-2.4.10 mdit-py-plugins-0.3.3 orjson-3.9.2 peft-0.4.0 pydub-0.25.1 python-multipart-0.0.6 safetensors-0.3.1 semantic-version-2.10.0 sentencepiece-0.1.99 shortuuid-1.0.11 starlette-0.27.0 svgwrite-1.4.3 timm-0.6.13 tokenizers-0.13.3 transformers-4.31.0 uvicorn-0.23.2 wavedrom-2.0.3.post3 websockets-11.0.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.0.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.0.1+cu118)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -e .\n",
        "!pip install ninja\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xqCUomawUq_",
        "outputId": "6c93d1bf-b5d0-479e-9191-893544700539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaVA\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzygL4wkwW6H",
        "outputId": "e6015a8b-a57a-4c1e-c4ff-b72e5c30a059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'vicuna-7b-v1.3'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 43 (delta 8), reused 0 (delta 0), pack-reused 26\u001b[K\n",
            "Unpacking objects: 100% (43/43), 8.10 KiB | 921.00 KiB/s, done.\n",
            "Filtering content: 100% (3/3), 4.55 GiB | 41.16 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model-00001-of-00002.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/lmsys/vicuna-7b-v1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQDU5NX_wYbG",
        "outputId": "aae28785-5bbd-4245-beba-4e84f9d57154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/ed/d7/8927aef63869d5d379adb63dc97f9cbc53830fdf85457b84a156fabcb231/wandb-0.15.8-py3-none-any.whl.metadata\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/67/50/742c2fb60989b76ccf7302c7b1d9e26505d7054c24f08cc7ec187faaaea7/GitPython-3.1.32-py3-none-any.whl.metadata\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/86/bb/ecb87fd214d5bbade07edf2ecdd829cf346e5b552689d6228112c6517286/sentry_sdk-1.29.2-py2.py3-none-any.whl.metadata\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=7098ec8949807a7b722bc488e20b8a4ca9acf3032695fb4c3e016c6f5a497cdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.8\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az7bkDIpwZyW",
        "outputId": "841f6328-5171-4c91-ef1f-cc62d0e1ea7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `key=\"a47d00a8e005c8adb0baa88002bc6f0ff3276cb7\"'\n",
            "/bin/bash: -c: line 1: `wandb.login(key=\"a47d00a8e005c8adb0baa88002bc6f0ff3276cb7\")'\n"
          ]
        }
      ],
      "source": [
        "#train\n",
        "!wandb.login(key=\"your key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv9U4VW3wbi_",
        "outputId": "a79add1e-3d38-4cf5-8af2-61b33d4d631a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-08-06 11:27:44,224] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-08-06 11:27:45.971854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\n",
            "Loading checkpoint shards: 100% 2/2 [00:09<00:00,  4.64s/it]\n",
            "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
            "Formatting inputs...Skip in lazy mode\n",
            "There were missing keys in the checkpoint model loaded: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'lm_head.weight'].\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcheckday0837\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaVA/wandb/run-20230806_112915-5eykdizh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfast-dragon-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/checkday0837/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/checkday0837/huggingface/runs/5eykdizh\u001b[0m\n",
            "{'loss': 0.4444, 'learning_rate': 0.001999999374142447, 'epoch': 1.0}\n",
            "{'loss': 0.5716, 'learning_rate': 0.0019999974965705727, 'epoch': 1.0}\n",
            "{'loss': 0.5551, 'learning_rate': 0.001999994367286727, 'epoch': 1.0}\n",
            "{'loss': 0.4674, 'learning_rate': 0.001999989986294826, 'epoch': 1.0}\n",
            "{'loss': 0.5396, 'learning_rate': 0.0019999843536003545, 'epoch': 1.0}\n",
            "{'loss': 0.4341, 'learning_rate': 0.001999977469210362, 'epoch': 1.0}\n",
            "{'loss': 0.4539, 'learning_rate': 0.001999969333133467, 'epoch': 1.0}\n",
            "{'loss': 0.6096, 'learning_rate': 0.0019999599453798525, 'epoch': 1.01}\n",
            "{'loss': 0.6921, 'learning_rate': 0.0019999493059612698, 'epoch': 1.01}\n",
            "{'loss': 0.4814, 'learning_rate': 0.0019999374148910363, 'epoch': 1.01}\n",
            "{'loss': 0.5949, 'learning_rate': 0.0019999242721840363, 'epoch': 1.01}\n",
            "{'loss': 0.5258, 'learning_rate': 0.001999909877856721, 'epoch': 1.01}\n",
            "{'loss': 0.5288, 'learning_rate': 0.0019998942319271076, 'epoch': 1.01}\n",
            "{'loss': 0.4757, 'learning_rate': 0.0019998773344147803, 'epoch': 1.01}\n",
            "{'loss': 0.5266, 'learning_rate': 0.00199985918534089, 'epoch': 1.01}\n",
            "{'loss': 0.542, 'learning_rate': 0.0019998397847281546, 'epoch': 1.01}\n",
            "{'loss': 0.4705, 'learning_rate': 0.0019998191326008577, 'epoch': 1.01}\n",
            "{'loss': 0.5146, 'learning_rate': 0.0019997972289848504, 'epoch': 1.01}\n",
            "{'loss': 0.5286, 'learning_rate': 0.001999774073907549, 'epoch': 1.01}\n",
            "{'loss': 0.4862, 'learning_rate': 0.0019997496673979376, 'epoch': 1.01}\n",
            "{'loss': 0.4776, 'learning_rate': 0.0019997240094865656, 'epoch': 1.01}\n",
            "{'loss': 0.5496, 'learning_rate': 0.0019996971002055504, 'epoch': 1.02}\n",
            "{'loss': 0.571, 'learning_rate': 0.001999668939588574, 'epoch': 1.02}\n",
            "{'loss': 0.5627, 'learning_rate': 0.0019996395276708854, 'epoch': 1.02}\n",
            "{'loss': 0.486, 'learning_rate': 0.0019996088644893004, 'epoch': 1.02}\n",
            "{'loss': 0.4933, 'learning_rate': 0.0019995769500822007, 'epoch': 1.02}\n",
            "{'loss': 0.5745, 'learning_rate': 0.0019995437844895336, 'epoch': 1.02}\n",
            "{'loss': 0.5052, 'learning_rate': 0.001999509367752813, 'epoch': 1.02}\n",
            "{'loss': 0.4863, 'learning_rate': 0.001999473699915119, 'epoch': 1.02}\n",
            "{'loss': 0.6312, 'learning_rate': 0.0019994367810210975, 'epoch': 1.02}\n",
            "{'loss': 0.5857, 'learning_rate': 0.0019993986111169603, 'epoch': 1.02}\n",
            "{'loss': 0.5998, 'learning_rate': 0.001999359190250485, 'epoch': 1.02}\n",
            "{'loss': 0.5491, 'learning_rate': 0.0019993185184710165, 'epoch': 1.02}\n",
            "{'loss': 0.5172, 'learning_rate': 0.0019992765958294634, 'epoch': 1.02}\n",
            "{'loss': 0.4719, 'learning_rate': 0.0019992334223783007, 'epoch': 1.02}\n",
            "{'loss': 0.5168, 'learning_rate': 0.0019991889981715695, 'epoch': 1.03}\n",
            "{'loss': 0.7261, 'learning_rate': 0.0019991433232648768, 'epoch': 1.03}\n",
            "{'loss': 0.5986, 'learning_rate': 0.0019990963977153937, 'epoch': 1.03}\n",
            "{'loss': 0.5377, 'learning_rate': 0.001999048221581858, 'epoch': 1.03}\n",
            "{'loss': 0.5803, 'learning_rate': 0.0019989987949245724, 'epoch': 1.03}\n",
            "{'loss': 0.5708, 'learning_rate': 0.001998948117805405, 'epoch': 1.03}\n",
            "{'loss': 0.4556, 'learning_rate': 0.0019988961902877894, 'epoch': 1.03}\n",
            "{'loss': 0.5163, 'learning_rate': 0.0019988430124367237, 'epoch': 1.03}\n",
            "{'loss': 0.5471, 'learning_rate': 0.001998788584318772, 'epoch': 1.03}\n",
            "{'loss': 0.5505, 'learning_rate': 0.0019987329060020616, 'epoch': 1.03}\n",
            "{'loss': 0.4893, 'learning_rate': 0.001998675977556287, 'epoch': 1.03}\n",
            "{'loss': 0.5506, 'learning_rate': 0.0019986177990527062, 'epoch': 1.03}\n",
            "{'loss': 0.4902, 'learning_rate': 0.0019985583705641415, 'epoch': 1.03}\n",
            "{'loss': 0.5044, 'learning_rate': 0.0019984976921649817, 'epoch': 1.03}\n",
            "{'loss': 0.6224, 'learning_rate': 0.0019984357639311777, 'epoch': 1.04}\n",
            "{'loss': 0.5237, 'learning_rate': 0.001998372585940246, 'epoch': 1.04}\n",
            "{'loss': 0.5698, 'learning_rate': 0.0019983081582712683, 'epoch': 1.04}\n",
            "{'loss': 0.4983, 'learning_rate': 0.001998242481004889, 'epoch': 1.04}\n",
            "{'loss': 0.5079, 'learning_rate': 0.0019981755542233175, 'epoch': 1.04}\n",
            "{'loss': 0.5334, 'learning_rate': 0.0019981073780103273, 'epoch': 1.04}\n",
            "{'loss': 0.5665, 'learning_rate': 0.0019980379524512553, 'epoch': 1.04}\n",
            "{'loss': 0.5479, 'learning_rate': 0.0019979672776330023, 'epoch': 1.04}\n",
            "{'loss': 0.5603, 'learning_rate': 0.0019978953536440337, 'epoch': 1.04}\n",
            "{'loss': 0.4707, 'learning_rate': 0.0019978221805743773, 'epoch': 1.04}\n",
            "{'loss': 0.5955, 'learning_rate': 0.001997747758515625, 'epoch': 1.04}\n",
            "{'loss': 0.4877, 'learning_rate': 0.0019976720875609323, 'epoch': 1.04}\n",
            "{'loss': 0.5686, 'learning_rate': 0.0019975951678050177, 'epoch': 1.04}\n",
            "{'loss': 0.4582, 'learning_rate': 0.0019975169993441625, 'epoch': 1.04}\n",
            "{'loss': 0.5533, 'learning_rate': 0.0019974375822762115, 'epoch': 1.05}\n",
            "{'loss': 0.5615, 'learning_rate': 0.0019973569167005723, 'epoch': 1.05}\n",
            "{'loss': 0.5252, 'learning_rate': 0.001997275002718215, 'epoch': 1.05}\n",
            "{'loss': 0.4631, 'learning_rate': 0.0019971918404316728, 'epoch': 1.05}\n",
            "{'loss': 0.4749, 'learning_rate': 0.001997107429945041, 'epoch': 1.05}\n",
            "{'loss': 0.543, 'learning_rate': 0.0019970217713639778, 'epoch': 1.05}\n",
            "{'loss': 0.5971, 'learning_rate': 0.001996934864795703, 'epoch': 1.05}\n",
            "{'loss': 0.6004, 'learning_rate': 0.001996846710348999, 'epoch': 1.05}\n",
            "{'loss': 0.5724, 'learning_rate': 0.00199675730813421, 'epoch': 1.05}\n",
            "{'loss': 0.5504, 'learning_rate': 0.0019966666582632423, 'epoch': 1.05}\n",
            "{'loss': 0.6556, 'learning_rate': 0.001996574760849563, 'epoch': 1.05}\n",
            "{'loss': 0.5279, 'learning_rate': 0.001996481616008203, 'epoch': 1.05}\n",
            "{'loss': 0.6032, 'learning_rate': 0.0019963872238557513, 'epoch': 1.05}\n",
            "{'loss': 0.6043, 'learning_rate': 0.0019962915845103614, 'epoch': 1.05}\n",
            "{'loss': 0.4764, 'learning_rate': 0.0019961946980917456, 'epoch': 1.06}\n",
            "{'loss': 0.518, 'learning_rate': 0.0019960965647211784, 'epoch': 1.06}\n",
            "{'loss': 0.562, 'learning_rate': 0.001995997184521495, 'epoch': 1.06}\n",
            "{'loss': 0.5187, 'learning_rate': 0.001995896557617091, 'epoch': 1.06}\n",
            "{'loss': 0.4908, 'learning_rate': 0.0019957946841339226, 'epoch': 1.06}\n",
            "{'loss': 0.4956, 'learning_rate': 0.001995691564199506, 'epoch': 1.06}\n",
            "{'loss': 0.5905, 'learning_rate': 0.0019955871979429186, 'epoch': 1.06}\n",
            "{'loss': 0.5787, 'learning_rate': 0.001995481585494797, 'epoch': 1.06}\n",
            "{'loss': 0.5221, 'learning_rate': 0.001995374726987338, 'epoch': 1.06}\n",
            "{'loss': 0.5815, 'learning_rate': 0.0019952666225542975, 'epoch': 1.06}\n",
            "{'loss': 0.473, 'learning_rate': 0.001995157272330992, 'epoch': 1.06}\n",
            "{'loss': 0.4733, 'learning_rate': 0.0019950466764542965, 'epoch': 1.06}\n",
            "{'loss': 0.4267, 'learning_rate': 0.0019949348350626456, 'epoch': 1.06}\n",
            "{'loss': 0.4852, 'learning_rate': 0.001994821748296033, 'epoch': 1.06}\n",
            "{'loss': 0.5021, 'learning_rate': 0.001994707416296011, 'epoch': 1.07}\n",
            "{'loss': 0.555, 'learning_rate': 0.001994591839205691, 'epoch': 1.07}\n",
            "{'loss': 0.5487, 'learning_rate': 0.001994475017169742, 'epoch': 1.07}\n",
            "{'loss': 0.5717, 'learning_rate': 0.0019943569503343924, 'epoch': 1.07}\n",
            "{'loss': 0.4778, 'learning_rate': 0.001994237638847428, 'epoch': 1.07}\n",
            "{'loss': 0.6229, 'learning_rate': 0.001994117082858193, 'epoch': 1.07}\n",
            "{'loss': 0.5471, 'learning_rate': 0.0019939952825175885, 'epoch': 1.07}\n",
            "{'loss': 0.5666, 'learning_rate': 0.0019938722379780746, 'epoch': 1.07}\n",
            "{'loss': 0.5055, 'learning_rate': 0.001993747949393668, 'epoch': 1.07}\n",
            "{'loss': 0.4969, 'learning_rate': 0.001993622416919942, 'epoch': 1.07}\n",
            "{'loss': 0.7393, 'learning_rate': 0.0019934956407140283, 'epoch': 1.07}\n",
            "{'loss': 0.6293, 'learning_rate': 0.001993367620934614, 'epoch': 1.07}\n",
            "{'loss': 0.4765, 'learning_rate': 0.001993238357741943, 'epoch': 1.07}\n",
            "{'loss': 0.5226, 'learning_rate': 0.001993107851297817, 'epoch': 1.07}\n",
            "{'loss': 0.5956, 'learning_rate': 0.001992976101765592, 'epoch': 1.08}\n",
            "{'loss': 0.4582, 'learning_rate': 0.0019928431093101814, 'epoch': 1.08}\n",
            "{'loss': 0.5158, 'learning_rate': 0.001992708874098054, 'epoch': 1.08}\n",
            "{'loss': 0.5411, 'learning_rate': 0.001992573396297234, 'epoch': 1.08}\n",
            "{'loss': 0.5298, 'learning_rate': 0.0019924366760773003, 'epoch': 1.08}\n",
            "{'loss': 0.5186, 'learning_rate': 0.001992298713609388, 'epoch': 1.08}\n",
            "{'loss': 0.4913, 'learning_rate': 0.001992159509066187, 'epoch': 1.08}\n",
            "{'loss': 0.6325, 'learning_rate': 0.001992019062621942, 'epoch': 1.08}\n",
            "{'loss': 0.5403, 'learning_rate': 0.001991877374452452, 'epoch': 1.08}\n",
            "{'loss': 0.5865, 'learning_rate': 0.0019917344447350693, 'epoch': 1.08}\n",
            "{'loss': 0.508, 'learning_rate': 0.001991590273648702, 'epoch': 1.08}\n",
            "{'loss': 0.5618, 'learning_rate': 0.0019914448613738106, 'epoch': 1.08}\n",
            "{'loss': 0.5062, 'learning_rate': 0.0019912982080924103, 'epoch': 1.08}\n",
            "{'loss': 0.5118, 'learning_rate': 0.001991150313988069, 'epoch': 1.08}\n",
            "{'loss': 0.477, 'learning_rate': 0.0019910011792459085, 'epoch': 1.09}\n",
            "{'loss': 0.5083, 'learning_rate': 0.0019908508040526024, 'epoch': 1.09}\n",
            "{'loss': 0.5194, 'learning_rate': 0.001990699188596378, 'epoch': 1.09}\n",
            "{'loss': 0.4162, 'learning_rate': 0.0019905463330670142, 'epoch': 1.09}\n",
            "{'loss': 0.4818, 'learning_rate': 0.001990392237655843, 'epoch': 1.09}\n",
            "{'loss': 0.4976, 'learning_rate': 0.001990236902555748, 'epoch': 1.09}\n",
            "{'loss': 0.5843, 'learning_rate': 0.0019900803279611643, 'epoch': 1.09}\n",
            "{'loss': 0.7092, 'learning_rate': 0.0019899225140680783, 'epoch': 1.09}\n",
            "{'loss': 0.5484, 'learning_rate': 0.0019897634610740286, 'epoch': 1.09}\n",
            "{'loss': 0.6131, 'learning_rate': 0.001989603169178104, 'epoch': 1.09}\n",
            "{'loss': 0.6007, 'learning_rate': 0.0019894416385809444, 'epoch': 1.09}\n",
            "{'loss': 0.6351, 'learning_rate': 0.0019892788694847404, 'epoch': 1.09}\n",
            "{'loss': 0.5009, 'learning_rate': 0.0019891148620932316, 'epoch': 1.09}\n",
            "{'loss': 0.55, 'learning_rate': 0.0019889496166117094, 'epoch': 1.09}\n",
            "{'loss': 0.5472, 'learning_rate': 0.0019887831332470137, 'epoch': 1.1}\n",
            "{'loss': 0.4995, 'learning_rate': 0.001988615412207534, 'epoch': 1.1}\n",
            "{'loss': 0.5285, 'learning_rate': 0.00198844645370321, 'epoch': 1.1}\n",
            "{'loss': 0.6778, 'learning_rate': 0.0019882762579455293, 'epoch': 1.1}\n",
            "{'loss': 0.6381, 'learning_rate': 0.001988104825147528, 'epoch': 1.1}\n",
            "{'loss': 0.524, 'learning_rate': 0.0019879321555237913, 'epoch': 1.1}\n",
            "{'loss': 0.4716, 'learning_rate': 0.001987758249290453, 'epoch': 1.1}\n",
            "{'loss': 0.5943, 'learning_rate': 0.001987583106665194, 'epoch': 1.1}\n",
            "{'loss': 0.502, 'learning_rate': 0.001987406727867242, 'epoch': 1.1}\n",
            "{'loss': 0.5617, 'learning_rate': 0.001987229113117374, 'epoch': 1.1}\n",
            "{'loss': 0.5748, 'learning_rate': 0.001987050262637913, 'epoch': 1.1}\n",
            "{'loss': 0.5574, 'learning_rate': 0.001986870176652728, 'epoch': 1.1}\n",
            "{'loss': 0.511, 'learning_rate': 0.0019866888553872366, 'epoch': 1.1}\n",
            "{'loss': 0.519, 'learning_rate': 0.0019865062990684005, 'epoch': 1.1}\n",
            "{'loss': 0.5173, 'learning_rate': 0.0019863225079247283, 'epoch': 1.11}\n",
            "{'loss': 0.5558, 'learning_rate': 0.0019861374821862746, 'epoch': 1.11}\n",
            "{'loss': 0.4476, 'learning_rate': 0.0019859512220846386, 'epoch': 1.11}\n",
            "{'loss': 0.5304, 'learning_rate': 0.0019857637278529647, 'epoch': 1.11}\n",
            "{'loss': 0.5383, 'learning_rate': 0.0019855749997259426, 'epoch': 1.11}\n",
            "{'loss': 0.6124, 'learning_rate': 0.001985385037939806, 'epoch': 1.11}\n",
            "{'loss': 0.5417, 'learning_rate': 0.001985193842732333, 'epoch': 1.11}\n",
            "{'loss': 0.5613, 'learning_rate': 0.001985001414342845, 'epoch': 1.11}\n",
            "{'loss': 0.4993, 'learning_rate': 0.001984807753012208, 'epoch': 1.11}\n",
            "{'loss': 0.5684, 'learning_rate': 0.0019846128589828312, 'epoch': 1.11}\n",
            "{'loss': 0.5821, 'learning_rate': 0.0019844167324986656, 'epoch': 1.11}\n",
            "{'loss': 0.59, 'learning_rate': 0.001984219373805206, 'epoch': 1.11}\n",
            "{'loss': 0.5463, 'learning_rate': 0.00198402078314949, 'epoch': 1.11}\n",
            "{'loss': 0.5518, 'learning_rate': 0.0019838209607800955, 'epoch': 1.11}\n",
            "{'loss': 0.4896, 'learning_rate': 0.0019836199069471435, 'epoch': 1.12}\n",
            "{'loss': 0.6398, 'learning_rate': 0.0019834176219022965, 'epoch': 1.12}\n",
            "{'loss': 0.6684, 'learning_rate': 0.001983214105898757, 'epoch': 1.12}\n",
            "{'loss': 0.5348, 'learning_rate': 0.0019830093591912696, 'epoch': 1.12}\n",
            "{'loss': 0.5733, 'learning_rate': 0.0019828033820361184, 'epoch': 1.12}\n",
            "{'loss': 0.6005, 'learning_rate': 0.001982596174691129, 'epoch': 1.12}\n",
            "{'loss': 0.5879, 'learning_rate': 0.0019823877374156645, 'epoch': 1.12}\n",
            "{'loss': 0.5263, 'learning_rate': 0.001982178070470631, 'epoch': 1.12}\n",
            "{'loss': 0.5136, 'learning_rate': 0.00198196717411847, 'epoch': 1.12}\n",
            "{'loss': 0.5624, 'learning_rate': 0.0019817550486231645, 'epoch': 1.12}\n",
            "{'loss': 0.5022, 'learning_rate': 0.001981541694250235, 'epoch': 1.12}\n",
            "{'loss': 0.5808, 'learning_rate': 0.0019813271112667395, 'epoch': 1.12}\n",
            "{'loss': 0.5794, 'learning_rate': 0.0019811112999412765, 'epoch': 1.12}\n",
            "{'loss': 0.5709, 'learning_rate': 0.0019808942605439796, 'epoch': 1.12}\n",
            "{'loss': 0.5777, 'learning_rate': 0.00198067599334652, 'epoch': 1.13}\n",
            "{'loss': 0.6204, 'learning_rate': 0.001980456498622106, 'epoch': 1.13}\n",
            "{'loss': 0.5875, 'learning_rate': 0.0019802357766454826, 'epoch': 1.13}\n",
            "{'loss': 0.6111, 'learning_rate': 0.0019800138276929308, 'epoch': 1.13}\n",
            "{'loss': 0.5294, 'learning_rate': 0.0019797906520422678, 'epoch': 1.13}\n",
            "{'loss': 0.4667, 'learning_rate': 0.0019795662499728457, 'epoch': 1.13}\n",
            "{'loss': 0.5518, 'learning_rate': 0.001979340621765552, 'epoch': 1.13}\n",
            "{'loss': 0.5412, 'learning_rate': 0.0019791137677028082, 'epoch': 1.13}\n",
            "{'loss': 0.5508, 'learning_rate': 0.001978885688068572, 'epoch': 1.13}\n",
            "{'loss': 0.5977, 'learning_rate': 0.001978656383148334, 'epoch': 1.13}\n",
            "{'loss': 0.5257, 'learning_rate': 0.001978425853229118, 'epoch': 1.13}\n",
            "{'loss': 0.5777, 'learning_rate': 0.0019781940985994823, 'epoch': 1.13}\n",
            "{'loss': 0.5452, 'learning_rate': 0.0019779611195495177, 'epoch': 1.13}\n",
            "{'loss': 0.4312, 'learning_rate': 0.001977726916370847, 'epoch': 1.13}\n",
            "{'loss': 0.4693, 'learning_rate': 0.0019774914893566265, 'epoch': 1.14}\n",
            "{'loss': 0.6503, 'learning_rate': 0.0019772548388015433, 'epoch': 1.14}\n",
            "{'loss': 0.531, 'learning_rate': 0.001977016965001817, 'epoch': 1.14}\n",
            "{'loss': 0.5299, 'learning_rate': 0.0019767778682551976, 'epoch': 1.14}\n",
            "{'loss': 0.4696, 'learning_rate': 0.001976537548860966, 'epoch': 1.14}\n",
            "{'loss': 0.5399, 'learning_rate': 0.0019762960071199333, 'epoch': 1.14}\n",
            "{'loss': 0.5932, 'learning_rate': 0.0019760532433344418, 'epoch': 1.14}\n",
            "{'loss': 0.5271, 'learning_rate': 0.0019758092578083617, 'epoch': 1.14}\n",
            "{'loss': 0.5597, 'learning_rate': 0.001975564050847094, 'epoch': 1.14}\n",
            "{'loss': 0.57, 'learning_rate': 0.0019753176227575676, 'epoch': 1.14}\n",
            "{'loss': 0.4385, 'learning_rate': 0.00197506997384824, 'epoch': 1.14}\n",
            "{'loss': 0.6483, 'learning_rate': 0.001974821104429098, 'epoch': 1.14}\n",
            "{'loss': 0.5148, 'learning_rate': 0.0019745710148116546, 'epoch': 1.14}\n",
            "{'loss': 0.5693, 'learning_rate': 0.001974319705308951, 'epoch': 1.14}\n",
            "{'loss': 0.5523, 'learning_rate': 0.0019740671762355547, 'epoch': 1.15}\n",
            "{'loss': 0.5191, 'learning_rate': 0.0019738134279075606, 'epoch': 1.15}\n",
            "{'loss': 0.5442, 'learning_rate': 0.0019735584606425886, 'epoch': 1.15}\n",
            "{'loss': 0.5588, 'learning_rate': 0.001973302274759786, 'epoch': 1.15}\n",
            "{'loss': 0.5225, 'learning_rate': 0.0019730448705798237, 'epoch': 1.15}\n",
            "{'loss': 0.5594, 'learning_rate': 0.0019727862484248994, 'epoch': 1.15}\n",
            "{'loss': 0.5294, 'learning_rate': 0.0019725264086187335, 'epoch': 1.15}\n",
            "{'loss': 0.5427, 'learning_rate': 0.0019722653514865716, 'epoch': 1.15}\n",
            "{'loss': 0.4667, 'learning_rate': 0.001972003077355183, 'epoch': 1.15}\n",
            "{'loss': 0.602, 'learning_rate': 0.00197173958655286, 'epoch': 1.15}\n",
            "{'loss': 0.4717, 'learning_rate': 0.0019714748794094185, 'epoch': 1.15}\n",
            "{'loss': 0.5384, 'learning_rate': 0.0019712089562561957, 'epoch': 1.15}\n",
            "{'loss': 0.5628, 'learning_rate': 0.001970941817426052, 'epoch': 1.15}\n",
            "{'loss': 0.5237, 'learning_rate': 0.0019706734632533693, 'epoch': 1.15}\n",
            "{'loss': 0.4152, 'learning_rate': 0.0019704038940740504, 'epoch': 1.16}\n",
            "{'loss': 0.6364, 'learning_rate': 0.001970133110225519, 'epoch': 1.16}\n",
            "{'loss': 0.5422, 'learning_rate': 0.0019698611120467194, 'epoch': 1.16}\n",
            "{'loss': 0.542, 'learning_rate': 0.001969587899878116, 'epoch': 1.16}\n",
            "{'loss': 0.5755, 'learning_rate': 0.0019693134740616926, 'epoch': 1.16}\n",
            "{'loss': 0.6652, 'learning_rate': 0.0019690378349409518, 'epoch': 1.16}\n",
            "{'loss': 0.5797, 'learning_rate': 0.0019687609828609154, 'epoch': 1.16}\n",
            "{'loss': 0.4723, 'learning_rate': 0.0019684829181681233, 'epoch': 1.16}\n",
            "{'loss': 0.499, 'learning_rate': 0.0019682036412106336, 'epoch': 1.16}\n",
            "{'loss': 0.5805, 'learning_rate': 0.0019679231523380214, 'epoch': 1.16}\n",
            "{'loss': 0.6166, 'learning_rate': 0.0019676414519013783, 'epoch': 1.16}\n",
            "{'loss': 0.4944, 'learning_rate': 0.0019673585402533132, 'epoch': 1.16}\n",
            "{'loss': 0.5831, 'learning_rate': 0.0019670744177479518, 'epoch': 1.16}\n",
            "{'loss': 0.6209, 'learning_rate': 0.0019667890847409335, 'epoch': 1.16}\n",
            "{'loss': 0.5616, 'learning_rate': 0.001966502541589414, 'epoch': 1.17}\n",
            "{'loss': 0.49, 'learning_rate': 0.001966214788652064, 'epoch': 1.17}\n",
            "{'loss': 0.5506, 'learning_rate': 0.0019659258262890683, 'epoch': 1.17}\n",
            "{'loss': 0.5427, 'learning_rate': 0.0019656356548621253, 'epoch': 1.17}\n",
            "{'loss': 0.5434, 'learning_rate': 0.001965344274734447, 'epoch': 1.17}\n",
            "{'loss': 0.549, 'learning_rate': 0.001965051686270758, 'epoch': 1.17}\n",
            "{'loss': 0.5409, 'learning_rate': 0.001964757889837296, 'epoch': 1.17}\n",
            "{'loss': 0.4622, 'learning_rate': 0.0019644628858018105, 'epoch': 1.17}\n",
            "{'loss': 0.5043, 'learning_rate': 0.0019641666745335625, 'epoch': 1.17}\n",
            "{'loss': 0.6617, 'learning_rate': 0.001963869256403324, 'epoch': 1.17}\n",
            "{'loss': 0.4812, 'learning_rate': 0.0019635706317833773, 'epoch': 1.17}\n",
            "{'loss': 0.5649, 'learning_rate': 0.0019632708010475165, 'epoch': 1.17}\n",
            "{'loss': 0.6449, 'learning_rate': 0.001962969764571043, 'epoch': 1.17}\n",
            "{'loss': 0.5548, 'learning_rate': 0.0019626675227307697, 'epoch': 1.17}\n",
            "{'loss': 0.515, 'learning_rate': 0.001962364075905017, 'epoch': 1.18}\n",
            "{'loss': 0.5002, 'learning_rate': 0.0019620594244736133, 'epoch': 1.18}\n",
            "{'loss': 0.5425, 'learning_rate': 0.001961753568817896, 'epoch': 1.18}\n",
            "{'loss': 0.4517, 'learning_rate': 0.0019614465093207092, 'epoch': 1.18}\n",
            "{'loss': 0.4792, 'learning_rate': 0.0019611382463664037, 'epoch': 1.18}\n",
            "{'loss': 0.5046, 'learning_rate': 0.001960828780340837, 'epoch': 1.18}\n",
            "{'loss': 0.401, 'learning_rate': 0.001960518111631372, 'epoch': 1.18}\n",
            "{'loss': 0.4851, 'learning_rate': 0.0019602062406268786, 'epoch': 1.18}\n",
            "{'loss': 0.566, 'learning_rate': 0.0019598931677177293, 'epoch': 1.18}\n",
            "{'loss': 0.5445, 'learning_rate': 0.0019595788932958024, 'epoch': 1.18}\n",
            "{'loss': 0.5774, 'learning_rate': 0.0019592634177544805, 'epoch': 1.18}\n",
            "{'loss': 0.5638, 'learning_rate': 0.0019589467414886483, 'epoch': 1.18}\n",
            "{'loss': 0.6653, 'learning_rate': 0.0019586288648946945, 'epoch': 1.18}\n",
            "{'loss': 0.5128, 'learning_rate': 0.0019583097883705105, 'epoch': 1.18}\n",
            "{'loss': 0.5789, 'learning_rate': 0.001957989512315489, 'epoch': 1.19}\n",
            "{'loss': 0.5712, 'learning_rate': 0.001957668037130524, 'epoch': 1.19}\n",
            "{'loss': 0.5205, 'learning_rate': 0.0019573453632180113, 'epoch': 1.19}\n",
            "{'loss': 0.6327, 'learning_rate': 0.0019570214909818465, 'epoch': 1.19}\n",
            "{'loss': 0.5781, 'learning_rate': 0.0019566964208274254, 'epoch': 1.19}\n",
            "{'loss': 0.5001, 'learning_rate': 0.0019563701531616433, 'epoch': 1.19}\n",
            "{'loss': 0.4312, 'learning_rate': 0.0019560426883928938, 'epoch': 1.19}\n",
            "{'loss': 0.5745, 'learning_rate': 0.00195571402693107, 'epoch': 1.19}\n",
            "{'loss': 0.5451, 'learning_rate': 0.001955384169187563, 'epoch': 1.19}\n",
            "{'loss': 0.4983, 'learning_rate': 0.00195505311557526, 'epoch': 1.19}\n",
            "{'loss': 0.4301, 'learning_rate': 0.0019547208665085455, 'epoch': 1.19}\n",
            "{'loss': 0.5216, 'learning_rate': 0.0019543874224033017, 'epoch': 1.19}\n",
            "{'loss': 0.5682, 'learning_rate': 0.0019540527836769046, 'epoch': 1.19}\n",
            "{'loss': 0.4862, 'learning_rate': 0.001953716950748227, 'epoch': 1.19}\n",
            "{'loss': 0.6122, 'learning_rate': 0.0019533799240376362, 'epoch': 1.2}\n",
            "{'loss': 0.5521, 'learning_rate': 0.0019530417039669932, 'epoch': 1.2}\n",
            "{'loss': 0.6174, 'learning_rate': 0.0019527022909596535, 'epoch': 1.2}\n",
            "{'loss': 0.6441, 'learning_rate': 0.0019523616854404656, 'epoch': 1.2}\n",
            "{'loss': 0.6574, 'learning_rate': 0.0019520198878357703, 'epoch': 1.2}\n",
            "{'loss': 0.6233, 'learning_rate': 0.0019516768985734006, 'epoch': 1.2}\n",
            "{'loss': 0.5523, 'learning_rate': 0.001951332718082682, 'epoch': 1.2}\n",
            "{'loss': 0.5885, 'learning_rate': 0.0019509873467944297, 'epoch': 1.2}\n",
            "{'loss': 0.5108, 'learning_rate': 0.001950640785140951, 'epoch': 1.2}\n",
            "{'loss': 0.562, 'learning_rate': 0.0019502930335560413, 'epoch': 1.2}\n",
            "{'loss': 0.6253, 'learning_rate': 0.0019499440924749876, 'epoch': 1.2}\n",
            "{'loss': 0.6782, 'learning_rate': 0.001949593962334564, 'epoch': 1.2}\n",
            "{'loss': 0.5274, 'learning_rate': 0.001949242643573034, 'epoch': 1.2}\n",
            "{'loss': 0.5459, 'learning_rate': 0.0019488901366301484, 'epoch': 1.2}\n",
            "{'loss': 0.609, 'learning_rate': 0.0019485364419471454, 'epoch': 1.21}\n",
            "{'loss': 0.5435, 'learning_rate': 0.0019481815599667505, 'epoch': 1.21}\n",
            "{'loss': 0.5241, 'learning_rate': 0.0019478254911331743, 'epoch': 1.21}\n",
            "{'loss': 0.5956, 'learning_rate': 0.0019474682358921137, 'epoch': 1.21}\n",
            "{'loss': 0.565, 'learning_rate': 0.0019471097946907505, 'epoch': 1.21}\n",
            "{'loss': 0.5691, 'learning_rate': 0.001946750167977751, 'epoch': 1.21}\n",
            "{'loss': 0.6251, 'learning_rate': 0.0019463893562032652, 'epoch': 1.21}\n",
            "{'loss': 0.5795, 'learning_rate': 0.0019460273598189272, 'epoch': 1.21}\n",
            "{'loss': 0.5345, 'learning_rate': 0.0019456641792778525, 'epoch': 1.21}\n",
            "{'loss': 0.5081, 'learning_rate': 0.0019452998150346402, 'epoch': 1.21}\n",
            "{'loss': 0.5551, 'learning_rate': 0.0019449342675453708, 'epoch': 1.21}\n",
            "{'loss': 0.6977, 'learning_rate': 0.0019445675372676049, 'epoch': 1.21}\n",
            "{'loss': 0.6101, 'learning_rate': 0.0019441996246603846, 'epoch': 1.21}\n",
            "{'loss': 0.5379, 'learning_rate': 0.001943830530184232, 'epoch': 1.21}\n",
            "{'loss': 0.5489, 'learning_rate': 0.0019434602543011478, 'epoch': 1.22}\n",
            "{'loss': 0.5445, 'learning_rate': 0.001943088797474612, 'epoch': 1.22}\n",
            "{'loss': 0.5976, 'learning_rate': 0.001942716160169583, 'epoch': 1.22}\n",
            "{'loss': 0.5333, 'learning_rate': 0.0019423423428524965, 'epoch': 1.22}\n",
            "{'loss': 0.4454, 'learning_rate': 0.001941967345991265, 'epoch': 1.22}\n",
            "{'loss': 0.5363, 'learning_rate': 0.0019415911700552777, 'epoch': 1.22}\n",
            "{'loss': 0.5627, 'learning_rate': 0.0019412138155154, 'epoch': 1.22}\n",
            "{'loss': 0.4553, 'learning_rate': 0.0019408352828439721, 'epoch': 1.22}\n",
            "{'loss': 0.5868, 'learning_rate': 0.0019404555725148094, 'epoch': 1.22}\n",
            "{'loss': 0.6038, 'learning_rate': 0.0019400746850032004, 'epoch': 1.22}\n",
            "{'loss': 0.5428, 'learning_rate': 0.0019396926207859084, 'epoch': 1.22}\n",
            "{'loss': 0.6272, 'learning_rate': 0.0019393093803411685, 'epoch': 1.22}\n",
            "{'loss': 0.5012, 'learning_rate': 0.001938924964148689, 'epoch': 1.22}\n",
            "{'loss': 0.5743, 'learning_rate': 0.0019385393726896489, 'epoch': 1.22}\n",
            "{'loss': 0.528, 'learning_rate': 0.0019381526064466991, 'epoch': 1.23}\n",
            "{'loss': 0.5653, 'learning_rate': 0.001937764665903961, 'epoch': 1.23}\n",
            "{'loss': 0.5729, 'learning_rate': 0.0019373755515470254, 'epoch': 1.23}\n",
            "{'loss': 0.4745, 'learning_rate': 0.0019369852638629523, 'epoch': 1.23}\n",
            "{'loss': 0.6547, 'learning_rate': 0.0019365938033402714, 'epoch': 1.23}\n",
            "{'loss': 0.5242, 'learning_rate': 0.0019362011704689795, 'epoch': 1.23}\n",
            "{'loss': 0.5203, 'learning_rate': 0.0019358073657405406, 'epoch': 1.23}\n",
            "{'loss': 0.5537, 'learning_rate': 0.0019354123896478867, 'epoch': 1.23}\n",
            "{'loss': 0.626, 'learning_rate': 0.001935016242685415, 'epoch': 1.23}\n",
            "{'loss': 0.5983, 'learning_rate': 0.0019346189253489886, 'epoch': 1.23}\n",
            "{'loss': 0.6068, 'learning_rate': 0.0019342204381359355, 'epoch': 1.23}\n",
            "{'loss': 0.507, 'learning_rate': 0.0019338207815450487, 'epoch': 1.23}\n",
            "{'loss': 0.4273, 'learning_rate': 0.001933419956076584, 'epoch': 1.23}\n",
            "{'loss': 0.5086, 'learning_rate': 0.0019330179622322606, 'epoch': 1.23}\n",
            "{'loss': 0.5028, 'learning_rate': 0.0019326148005152604, 'epoch': 1.23}\n",
            "{'loss': 0.564, 'learning_rate': 0.0019322104714302268, 'epoch': 1.24}\n",
            "{'loss': 0.5798, 'learning_rate': 0.0019318049754832654, 'epoch': 1.24}\n",
            "{'loss': 0.5526, 'learning_rate': 0.0019313983131819406, 'epoch': 1.24}\n",
            "{'loss': 0.6639, 'learning_rate': 0.0019309904850352785, 'epoch': 1.24}\n",
            "{'loss': 0.5047, 'learning_rate': 0.0019305814915537632, 'epoch': 1.24}\n",
            "{'loss': 0.611, 'learning_rate': 0.0019301713332493386, 'epoch': 1.24}\n",
            "{'loss': 0.5591, 'learning_rate': 0.001929760010635405, 'epoch': 1.24}\n",
            "{'loss': 0.5457, 'learning_rate': 0.0019293475242268224, 'epoch': 1.24}\n",
            "{'loss': 0.513, 'learning_rate': 0.0019289338745399056, 'epoch': 1.24}\n",
            "{'loss': 0.5957, 'learning_rate': 0.0019285190620924265, 'epoch': 1.24}\n",
            "{'loss': 0.5783, 'learning_rate': 0.0019281030874036117, 'epoch': 1.24}\n",
            "{'loss': 0.7084, 'learning_rate': 0.001927685950994143, 'epoch': 1.24}\n",
            "{'loss': 0.5595, 'learning_rate': 0.0019272676533861565, 'epoch': 1.24}\n",
            "{'loss': 0.5232, 'learning_rate': 0.0019268481951032418, 'epoch': 1.24}\n",
            "{'loss': 0.6026, 'learning_rate': 0.001926427576670441, 'epoch': 1.25}\n",
            "{'loss': 0.5404, 'learning_rate': 0.0019260057986142485, 'epoch': 1.25}\n",
            "{'loss': 0.6179, 'learning_rate': 0.0019255828614626106, 'epoch': 1.25}\n",
            "{'loss': 0.5848, 'learning_rate': 0.0019251587657449236, 'epoch': 1.25}\n",
            "{'loss': 0.5377, 'learning_rate': 0.0019247335119920348, 'epoch': 1.25}\n",
            "{'loss': 0.5803, 'learning_rate': 0.0019243071007362406, 'epoch': 1.25}\n",
            "{'loss': 0.662, 'learning_rate': 0.0019238795325112869, 'epoch': 1.25}\n",
            "{'loss': 0.56, 'learning_rate': 0.0019234508078523668, 'epoch': 1.25}\n",
            "{'loss': 0.4343, 'learning_rate': 0.0019230209272961215, 'epoch': 1.25}\n",
            "{'loss': 0.5571, 'learning_rate': 0.0019225898913806387, 'epoch': 1.25}\n",
            "{'loss': 0.4475, 'learning_rate': 0.0019221577006454532, 'epoch': 1.25}\n",
            "{'loss': 0.5417, 'learning_rate': 0.0019217243556315445, 'epoch': 1.25}\n",
            "{'loss': 0.7052, 'learning_rate': 0.0019212898568813368, 'epoch': 1.25}\n",
            "{'loss': 0.5808, 'learning_rate': 0.001920854204938699, 'epoch': 1.25}\n",
            "{'loss': 0.6322, 'learning_rate': 0.0019204174003489432, 'epoch': 1.26}\n",
            "{'loss': 0.444, 'learning_rate': 0.0019199794436588243, 'epoch': 1.26}\n",
            "{'loss': 0.5249, 'learning_rate': 0.0019195403354165391, 'epoch': 1.26}\n",
            "{'loss': 0.4965, 'learning_rate': 0.0019191000761717262, 'epoch': 1.26}\n",
            "{'loss': 0.4954, 'learning_rate': 0.0019186586664754648, 'epoch': 1.26}\n",
            "{'loss': 0.6056, 'learning_rate': 0.001918216106880274, 'epoch': 1.26}\n",
            "{'loss': 0.5551, 'learning_rate': 0.0019177723979401122, 'epoch': 1.26}\n",
            "{'loss': 0.5321, 'learning_rate': 0.0019173275402103772, 'epoch': 1.26}\n",
            "{'loss': 0.5626, 'learning_rate': 0.0019168815342479031, 'epoch': 1.26}\n",
            "{'loss': 0.5129, 'learning_rate': 0.001916434380610963, 'epoch': 1.26}\n",
            "{'loss': 0.4713, 'learning_rate': 0.001915986079859266, 'epoch': 1.26}\n",
            "{'loss': 0.5426, 'learning_rate': 0.0019155366325539563, 'epoch': 1.26}\n",
            "{'loss': 0.5446, 'learning_rate': 0.0019150860392576146, 'epoch': 1.26}\n",
            "{'loss': 0.5223, 'learning_rate': 0.0019146343005342545, 'epoch': 1.26}\n",
            "{'loss': 0.5624, 'learning_rate': 0.001914181416949325, 'epoch': 1.27}\n",
            "{'loss': 0.5843, 'learning_rate': 0.0019137273890697068, 'epoch': 1.27}\n",
            "{'loss': 0.5912, 'learning_rate': 0.0019132722174637139, 'epoch': 1.27}\n",
            "{'loss': 0.4751, 'learning_rate': 0.001912815902701091, 'epoch': 1.27}\n",
            "{'loss': 0.4976, 'learning_rate': 0.0019123584453530143, 'epoch': 1.27}\n",
            "{'loss': 0.6051, 'learning_rate': 0.0019118998459920903, 'epoch': 1.27}\n",
            "{'loss': 0.5337, 'learning_rate': 0.0019114401051923543, 'epoch': 1.27}\n",
            "{'loss': 0.5321, 'learning_rate': 0.0019109792235292714, 'epoch': 1.27}\n",
            "{'loss': 0.6086, 'learning_rate': 0.0019105172015797338, 'epoch': 1.27}\n",
            "{'loss': 0.6096, 'learning_rate': 0.0019100540399220613, 'epoch': 1.27}\n",
            "{'loss': 0.5229, 'learning_rate': 0.0019095897391360004, 'epoch': 1.27}\n",
            "{'loss': 0.4437, 'learning_rate': 0.0019091242998027238, 'epoch': 1.27}\n",
            "{'loss': 0.5972, 'learning_rate': 0.0019086577225048283, 'epoch': 1.27}\n",
            "{'loss': 0.5182, 'learning_rate': 0.0019081900078263359, 'epoch': 1.27}\n",
            "{'loss': 0.5735, 'learning_rate': 0.0019077211563526925, 'epoch': 1.28}\n",
            "{'loss': 0.5668, 'learning_rate': 0.0019072511686707664, 'epoch': 1.28}\n",
            "{'loss': 0.6405, 'learning_rate': 0.0019067800453688483, 'epoch': 1.28}\n",
            "{'loss': 0.54, 'learning_rate': 0.0019063077870366501, 'epoch': 1.28}\n",
            "{'loss': 0.5008, 'learning_rate': 0.001905834394265305, 'epoch': 1.28}\n",
            "{'loss': 0.5939, 'learning_rate': 0.0019053598676473653, 'epoch': 1.28}\n",
            "{'loss': 0.5267, 'learning_rate': 0.0019048842077768041, 'epoch': 1.28}\n",
            "{'loss': 0.558, 'learning_rate': 0.0019044074152490113, 'epoch': 1.28}\n",
            "{'loss': 0.5262, 'learning_rate': 0.0019039294906607956, 'epoch': 1.28}\n",
            "{'loss': 0.7598, 'learning_rate': 0.0019034504346103823, 'epoch': 1.28}\n",
            "{'loss': 0.6519, 'learning_rate': 0.001902970247697413, 'epoch': 1.28}\n",
            "{'loss': 0.5729, 'learning_rate': 0.0019024889305229455, 'epoch': 1.28}\n",
            "{'loss': 0.4365, 'learning_rate': 0.0019020064836894513, 'epoch': 1.28}\n",
            "{'loss': 0.6122, 'learning_rate': 0.0019015229078008162, 'epoch': 1.28}\n",
            "{'loss': 0.5118, 'learning_rate': 0.0019010382034623397, 'epoch': 1.29}\n",
            "{'loss': 0.5443, 'learning_rate': 0.0019005523712807336, 'epoch': 1.29}\n",
            "{'loss': 0.5535, 'learning_rate': 0.0019000654118641212, 'epoch': 1.29}\n",
            "{'loss': 0.5917, 'learning_rate': 0.0018995773258220373, 'epoch': 1.29}\n",
            "{'loss': 0.6192, 'learning_rate': 0.0018990881137654258, 'epoch': 1.29}\n",
            "{'loss': 0.5752, 'learning_rate': 0.001898597776306642, 'epoch': 1.29}\n",
            "{'loss': 0.5483, 'learning_rate': 0.0018981063140594476, 'epoch': 1.29}\n",
            "{'loss': 0.4993, 'learning_rate': 0.0018976137276390142, 'epoch': 1.29}\n",
            "{'loss': 0.6506, 'learning_rate': 0.0018971200176619188, 'epoch': 1.29}\n",
            "{'loss': 0.5081, 'learning_rate': 0.0018966251847461462, 'epoch': 1.29}\n",
            "{'loss': 0.5716, 'learning_rate': 0.0018961292295110864, 'epoch': 1.29}\n",
            "{'loss': 0.5023, 'learning_rate': 0.0018956321525775335, 'epoch': 1.29}\n",
            "{'loss': 0.6032, 'learning_rate': 0.0018951339545676866, 'epoch': 1.29}\n",
            "{'loss': 0.5271, 'learning_rate': 0.0018946346361051474, 'epoch': 1.29}\n",
            "{'loss': 0.6577, 'learning_rate': 0.0018941341978149207, 'epoch': 1.3}\n",
            "{'loss': 0.5661, 'learning_rate': 0.0018936326403234123, 'epoch': 1.3}\n",
            "{'loss': 0.5036, 'learning_rate': 0.0018931299642584297, 'epoch': 1.3}\n",
            "{'loss': 0.6444, 'learning_rate': 0.0018926261702491797, 'epoch': 1.3}\n",
            "{'loss': 0.6914, 'learning_rate': 0.0018921212589262687, 'epoch': 1.3}\n",
            "{'loss': 0.5939, 'learning_rate': 0.001891615230921703, 'epoch': 1.3}\n",
            "{'loss': 0.5668, 'learning_rate': 0.0018911080868688841, 'epoch': 1.3}\n",
            "{'loss': 0.6968, 'learning_rate': 0.0018905998274026125, 'epoch': 1.3}\n",
            "{'loss': 0.498, 'learning_rate': 0.0018900904531590846, 'epoch': 1.3}\n",
            "{'loss': 0.6253, 'learning_rate': 0.001889579964775891, 'epoch': 1.3}\n",
            "{'loss': 0.5205, 'learning_rate': 0.0018890683628920186, 'epoch': 1.3}\n",
            "{'loss': 0.5035, 'learning_rate': 0.0018885556481478465, 'epoch': 1.3}\n",
            "{'loss': 0.7307, 'learning_rate': 0.0018880418211851477, 'epoch': 1.3}\n",
            "{'loss': 0.5951, 'learning_rate': 0.0018875268826470872, 'epoch': 1.3}\n",
            "{'loss': 0.5621, 'learning_rate': 0.0018870108331782217, 'epoch': 1.31}\n",
            "{'loss': 0.5823, 'learning_rate': 0.0018864936734244977, 'epoch': 1.31}\n",
            "{'loss': 0.6693, 'learning_rate': 0.001885975404033252, 'epoch': 1.31}\n",
            "{'loss': 0.594, 'learning_rate': 0.0018854560256532098, 'epoch': 1.31}\n",
            "{'loss': 0.5343, 'learning_rate': 0.0018849355389344855, 'epoch': 1.31}\n",
            "{'loss': 0.5578, 'learning_rate': 0.0018844139445285803, 'epoch': 1.31}\n",
            "{'loss': 0.6092, 'learning_rate': 0.001883891243088381, 'epoch': 1.31}\n",
            "{'loss': 0.5846, 'learning_rate': 0.0018833674352681613, 'epoch': 1.31}\n",
            "{'loss': 0.5637, 'learning_rate': 0.0018828425217235795, 'epoch': 1.31}\n",
            "{'loss': 0.5261, 'learning_rate': 0.001882316503111678, 'epoch': 1.31}\n",
            "{'loss': 0.5431, 'learning_rate': 0.0018817893800908818, 'epoch': 1.31}\n",
            "{'loss': 0.5534, 'learning_rate': 0.0018812611533209989, 'epoch': 1.31}\n",
            "{'loss': 0.5904, 'learning_rate': 0.0018807318234632185, 'epoch': 1.31}\n",
            "{'loss': 0.5703, 'learning_rate': 0.001880201391180111, 'epoch': 1.31}\n",
            "{'loss': 0.4962, 'learning_rate': 0.0018796698571356268, 'epoch': 1.32}\n",
            "{'loss': 0.5387, 'learning_rate': 0.0018791372219950947, 'epoch': 1.32}\n",
            "{'loss': 0.5577, 'learning_rate': 0.001878603486425222, 'epoch': 1.32}\n",
            "{'loss': 0.703, 'learning_rate': 0.001878068651094094, 'epoch': 1.32}\n",
            "{'loss': 0.5445, 'learning_rate': 0.0018775327166711722, 'epoch': 1.32}\n",
            "{'loss': 0.6179, 'learning_rate': 0.0018769956838272934, 'epoch': 1.32}\n",
            "{'loss': 0.6911, 'learning_rate': 0.0018764575532346699, 'epoch': 1.32}\n",
            "{'loss': 0.5231, 'learning_rate': 0.001875918325566888, 'epoch': 1.32}\n",
            "{'loss': 0.5511, 'learning_rate': 0.0018753780014989067, 'epoch': 1.32}\n",
            "{'loss': 0.5022, 'learning_rate': 0.0018748365817070586, 'epoch': 1.32}\n",
            "{'loss': 0.6211, 'learning_rate': 0.0018742940668690462, 'epoch': 1.32}\n",
            "{'loss': 0.4607, 'learning_rate': 0.001873750457663944, 'epoch': 1.32}\n",
            "{'loss': 0.5125, 'learning_rate': 0.0018732057547721959, 'epoch': 1.32}\n",
            "{'loss': 0.5539, 'learning_rate': 0.0018726599588756144, 'epoch': 1.32}\n",
            "{'loss': 0.4269, 'learning_rate': 0.0018721130706573807, 'epoch': 1.33}\n",
            "{'loss': 0.5708, 'learning_rate': 0.0018715650908020427, 'epoch': 1.33}\n",
            "{'loss': 0.5398, 'learning_rate': 0.0018710160199955155, 'epoch': 1.33}\n",
            "{'loss': 0.6565, 'learning_rate': 0.0018704658589250795, 'epoch': 1.33}\n",
            "{'loss': 0.5264, 'learning_rate': 0.001869914608279379, 'epoch': 1.33}\n",
            "{'loss': 0.4732, 'learning_rate': 0.0018693622687484229, 'epoch': 1.33}\n",
            "{'loss': 0.5907, 'learning_rate': 0.0018688088410235833, 'epoch': 1.33}\n",
            "{'loss': 0.7014, 'learning_rate': 0.0018682543257975937, 'epoch': 1.33}\n",
            "{'loss': 0.5483, 'learning_rate': 0.0018676987237645493, 'epoch': 1.33}\n",
            "{'loss': 0.5975, 'learning_rate': 0.0018671420356199055, 'epoch': 1.33}\n",
            "{'loss': 0.5467, 'learning_rate': 0.0018665842620604776, 'epoch': 1.33}\n",
            "{'loss': 0.6194, 'learning_rate': 0.001866025403784439, 'epoch': 1.33}\n",
            "{'loss': 0.5365, 'learning_rate': 0.0018654654614913202, 'epoch': 1.33}\n",
            "{'loss': 0.5501, 'learning_rate': 0.001864904435882011, 'epoch': 1.33}\n",
            "{'loss': 0.5749, 'learning_rate': 0.0018643423276587545, 'epoch': 1.34}\n",
            "{'loss': 0.6213, 'learning_rate': 0.0018637791375251502, 'epoch': 1.34}\n",
            "{'loss': 0.4872, 'learning_rate': 0.001863214866186152, 'epoch': 1.34}\n",
            "{'loss': 0.5456, 'learning_rate': 0.0018626495143480668, 'epoch': 1.34}\n",
            "{'loss': 0.5944, 'learning_rate': 0.001862083082718554, 'epoch': 1.34}\n",
            "{'loss': 0.6336, 'learning_rate': 0.0018615155720066246, 'epoch': 1.34}\n",
            "{'loss': 0.7028, 'learning_rate': 0.00186094698292264, 'epoch': 1.34}\n",
            "{'loss': 0.5782, 'learning_rate': 0.0018603773161783123, 'epoch': 1.34}\n",
            "{'loss': 0.5482, 'learning_rate': 0.001859806572486702, 'epoch': 1.34}\n",
            "{'loss': 0.4759, 'learning_rate': 0.001859234752562217, 'epoch': 1.34}\n",
            "{'loss': 0.6268, 'learning_rate': 0.0018586618571206134, 'epoch': 1.34}\n",
            "{'loss': 0.6564, 'learning_rate': 0.0018580878868789928, 'epoch': 1.34}\n",
            "{'loss': 0.4363, 'learning_rate': 0.0018575128425558024, 'epoch': 1.34}\n",
            "{'loss': 0.5339, 'learning_rate': 0.0018569367248708341, 'epoch': 1.34}\n",
            "{'loss': 0.531, 'learning_rate': 0.0018563595345452233, 'epoch': 1.35}\n",
            "{'loss': 0.5199, 'learning_rate': 0.0018557812723014476, 'epoch': 1.35}\n",
            "{'loss': 0.5782, 'learning_rate': 0.0018552019388633264, 'epoch': 1.35}\n",
            "{'loss': 0.5022, 'learning_rate': 0.0018546215349560203, 'epoch': 1.35}\n",
            "{'loss': 0.3997, 'learning_rate': 0.0018540400613060299, 'epoch': 1.35}\n",
            "{'loss': 0.6442, 'learning_rate': 0.001853457518641194, 'epoch': 1.35}\n",
            "{'loss': 0.565, 'learning_rate': 0.0018528739076906905, 'epoch': 1.35}\n",
            "{'loss': 0.485, 'learning_rate': 0.0018522892291850336, 'epoch': 1.35}\n",
            "{'loss': 0.482, 'learning_rate': 0.0018517034838560747, 'epoch': 1.35}\n",
            "{'loss': 0.5022, 'learning_rate': 0.0018511166724369996, 'epoch': 1.35}\n",
            "{'loss': 0.5876, 'learning_rate': 0.0018505287956623296, 'epoch': 1.35}\n",
            "{'loss': 0.5254, 'learning_rate': 0.0018499398542679187, 'epoch': 1.35}\n",
            "{'loss': 0.5515, 'learning_rate': 0.0018493498489909534, 'epoch': 1.35}\n",
            "{'loss': 0.5591, 'learning_rate': 0.0018487587805699527, 'epoch': 1.35}\n",
            "{'loss': 0.4939, 'learning_rate': 0.0018481666497447656, 'epoch': 1.36}\n",
            "{'loss': 0.5326, 'learning_rate': 0.001847573457256571, 'epoch': 1.36}\n",
            "{'loss': 0.5029, 'learning_rate': 0.0018469792038478775, 'epoch': 1.36}\n",
            "{'loss': 0.4839, 'learning_rate': 0.0018463838902625204, 'epoch': 1.36}\n",
            "{'loss': 0.6884, 'learning_rate': 0.0018457875172456634, 'epoch': 1.36}\n",
            "{'loss': 0.5833, 'learning_rate': 0.001845190085543795, 'epoch': 1.36}\n",
            "{'loss': 0.4577, 'learning_rate': 0.0018445915959047294, 'epoch': 1.36}\n",
            "{'loss': 0.5405, 'learning_rate': 0.0018439920490776062, 'epoch': 1.36}\n",
            "{'loss': 0.4935, 'learning_rate': 0.0018433914458128857, 'epoch': 1.36}\n",
            "{'loss': 0.6098, 'learning_rate': 0.0018427897868623533, 'epoch': 1.36}\n",
            "{'loss': 0.6565, 'learning_rate': 0.001842187072979114, 'epoch': 1.36}\n",
            "{'loss': 0.6063, 'learning_rate': 0.0018415833049175941, 'epoch': 1.36}\n",
            "{'loss': 0.6119, 'learning_rate': 0.0018409784834335387, 'epoch': 1.36}\n",
            "{'loss': 0.515, 'learning_rate': 0.001840372609284013, 'epoch': 1.36}\n",
            "{'loss': 0.5562, 'learning_rate': 0.0018397656832273982, 'epoch': 1.37}\n",
            "{'loss': 0.495, 'learning_rate': 0.0018391577060233924, 'epoch': 1.37}\n",
            "{'loss': 0.5738, 'learning_rate': 0.0018385486784330105, 'epoch': 1.37}\n",
            "{'loss': 0.594, 'learning_rate': 0.0018379386012185813, 'epoch': 1.37}\n",
            "{'loss': 0.5676, 'learning_rate': 0.0018373274751437476, 'epoch': 1.37}\n",
            "{'loss': 0.5592, 'learning_rate': 0.0018367153009734654, 'epoch': 1.37}\n",
            "{'loss': 0.5252, 'learning_rate': 0.0018361020794740023, 'epoch': 1.37}\n",
            "{'loss': 0.6328, 'learning_rate': 0.0018354878114129364, 'epoch': 1.37}\n",
            "{'loss': 0.5433, 'learning_rate': 0.0018348724975591568, 'epoch': 1.37}\n",
            "{'loss': 0.5348, 'learning_rate': 0.0018342561386828612, 'epoch': 1.37}\n",
            "{'loss': 0.5607, 'learning_rate': 0.0018336387355555553, 'epoch': 1.37}\n",
            "{'loss': 0.6356, 'learning_rate': 0.0018330202889500517, 'epoch': 1.37}\n",
            "{'loss': 0.5013, 'learning_rate': 0.0018324007996404692, 'epoch': 1.37}\n",
            "{'loss': 0.6089, 'learning_rate': 0.0018317802684022324, 'epoch': 1.37}\n",
            "{'loss': 0.4989, 'learning_rate': 0.0018311586960120694, 'epoch': 1.38}\n",
            "{'loss': 0.5414, 'learning_rate': 0.0018305360832480118, 'epoch': 1.38}\n",
            "{'loss': 0.5463, 'learning_rate': 0.0018299124308893933, 'epoch': 1.38}\n",
            "{'loss': 0.6108, 'learning_rate': 0.0018292877397168487, 'epoch': 1.38}\n",
            "{'loss': 0.5968, 'learning_rate': 0.001828662010512314, 'epoch': 1.38}\n",
            "{'loss': 0.4914, 'learning_rate': 0.0018280352440590236, 'epoch': 1.38}\n",
            "{'loss': 0.5063, 'learning_rate': 0.0018274074411415106, 'epoch': 1.38}\n",
            "{'loss': 0.5269, 'learning_rate': 0.0018267786025456052, 'epoch': 1.38}\n",
            "{'loss': 0.5528, 'learning_rate': 0.0018261487290584344, 'epoch': 1.38}\n",
            "{'loss': 0.4549, 'learning_rate': 0.00182551782146842, 'epoch': 1.38}\n",
            "{'loss': 0.5239, 'learning_rate': 0.0018248858805652792, 'epoch': 1.38}\n",
            "{'loss': 0.5042, 'learning_rate': 0.0018242529071400213, 'epoch': 1.38}\n",
            "{'loss': 0.5113, 'learning_rate': 0.0018236189019849491, 'epoch': 1.38}\n",
            "{'loss': 0.6055, 'learning_rate': 0.0018229838658936565, 'epoch': 1.38}\n",
            "{'loss': 0.5086, 'learning_rate': 0.0018223477996610273, 'epoch': 1.39}\n",
            "{'loss': 0.545, 'learning_rate': 0.0018217107040832358, 'epoch': 1.39}\n",
            "{'loss': 0.473, 'learning_rate': 0.0018210725799577437, 'epoch': 1.39}\n",
            "{'loss': 0.5673, 'learning_rate': 0.0018204334280833006, 'epoch': 1.39}\n",
            "{'loss': 0.6038, 'learning_rate': 0.0018197932492599427, 'epoch': 1.39}\n",
            "{'loss': 0.5511, 'learning_rate': 0.0018191520442889918, 'epoch': 1.39}\n",
            "{'loss': 0.5136, 'learning_rate': 0.0018185098139730535, 'epoch': 1.39}\n",
            "{'loss': 0.6427, 'learning_rate': 0.001817866559116017, 'epoch': 1.39}\n",
            "{'loss': 0.4566, 'learning_rate': 0.0018172222805230547, 'epoch': 1.39}\n",
            "{'loss': 0.5827, 'learning_rate': 0.0018165769790006196, 'epoch': 1.39}\n",
            "{'loss': 0.5615, 'learning_rate': 0.001815930655356445, 'epoch': 1.39}\n",
            "{'loss': 0.5451, 'learning_rate': 0.0018152833103995444, 'epoch': 1.39}\n",
            "{'loss': 0.4889, 'learning_rate': 0.001814634944940209, 'epoch': 1.39}\n",
            "{'loss': 0.5878, 'learning_rate': 0.001813985559790008, 'epoch': 1.39}\n",
            "{'loss': 0.5928, 'learning_rate': 0.0018133351557617862, 'epoch': 1.4}\n",
            "{'loss': 0.5776, 'learning_rate': 0.0018126837336696643, 'epoch': 1.4}\n",
            "{'loss': 0.4984, 'learning_rate': 0.0018120312943290372, 'epoch': 1.4}\n",
            "{'loss': 0.5995, 'learning_rate': 0.0018113778385565732, 'epoch': 1.4}\n",
            "{'loss': 0.5257, 'learning_rate': 0.0018107233671702124, 'epoch': 1.4}\n",
            "{'loss': 0.6649, 'learning_rate': 0.0018100678809891667, 'epoch': 1.4}\n",
            "{'loss': 0.5297, 'learning_rate': 0.0018094113808339181, 'epoch': 1.4}\n",
            "{'loss': 0.58, 'learning_rate': 0.0018087538675262181, 'epoch': 1.4}\n",
            "{'loss': 0.5357, 'learning_rate': 0.0018080953418890854, 'epoch': 1.4}\n",
            "{'loss': 0.5762, 'learning_rate': 0.001807435804746807, 'epoch': 1.4}\n",
            "{'loss': 0.5946, 'learning_rate': 0.001806775256924935, 'epoch': 1.4}\n",
            "{'loss': 0.6316, 'learning_rate': 0.0018061136992502873, 'epoch': 1.4}\n",
            "{'loss': 0.5467, 'learning_rate': 0.001805451132550946, 'epoch': 1.4}\n",
            "{'loss': 0.5985, 'learning_rate': 0.0018047875576562556, 'epoch': 1.4}\n",
            "{'loss': 0.5578, 'learning_rate': 0.0018041229753968224, 'epoch': 1.41}\n",
            "{'loss': 0.5702, 'learning_rate': 0.0018034573866045145, 'epoch': 1.41}\n",
            "{'loss': 0.5214, 'learning_rate': 0.0018027907921124595, 'epoch': 1.41}\n",
            "{'loss': 0.6031, 'learning_rate': 0.0018021231927550438, 'epoch': 1.41}\n",
            "{'loss': 0.5991, 'learning_rate': 0.0018014545893679115, 'epoch': 1.41}\n",
            "{'loss': 0.5787, 'learning_rate': 0.0018007849827879631, 'epoch': 1.41}\n",
            "{'loss': 0.5072, 'learning_rate': 0.001800114373853356, 'epoch': 1.41}\n",
            "{'loss': 0.5557, 'learning_rate': 0.0017994427634035013, 'epoch': 1.41}\n",
            "{'loss': 0.5628, 'learning_rate': 0.0017987701522790636, 'epoch': 1.41}\n",
            "{'loss': 0.4819, 'learning_rate': 0.0017980965413219607, 'epoch': 1.41}\n",
            "{'loss': 0.549, 'learning_rate': 0.0017974219313753616, 'epoch': 1.41}\n",
            "{'loss': 0.5262, 'learning_rate': 0.001796746323283686, 'epoch': 1.41}\n",
            "{'loss': 0.6278, 'learning_rate': 0.0017960697178926025, 'epoch': 1.41}\n",
            "{'loss': 0.5242, 'learning_rate': 0.0017953921160490278, 'epoch': 1.41}\n",
            "{'loss': 0.5514, 'learning_rate': 0.0017947135186011274, 'epoch': 1.42}\n",
            "{'loss': 0.7077, 'learning_rate': 0.0017940339263983111, 'epoch': 1.42}\n",
            "{'loss': 0.4231, 'learning_rate': 0.0017933533402912352, 'epoch': 1.42}\n",
            "{'loss': 0.5698, 'learning_rate': 0.0017926717611317993, 'epoch': 1.42}\n",
            "{'loss': 0.5248, 'learning_rate': 0.0017919891897731465, 'epoch': 1.42}\n",
            "{'loss': 0.5249, 'learning_rate': 0.001791305627069662, 'epoch': 1.42}\n",
            "{'loss': 0.6501, 'learning_rate': 0.001790621073876971, 'epoch': 1.42}\n",
            "{'loss': 0.517, 'learning_rate': 0.0017899355310519393, 'epoch': 1.42}\n",
            "{'loss': 0.6121, 'learning_rate': 0.0017892489994526712, 'epoch': 1.42}\n",
            "{'loss': 0.5831, 'learning_rate': 0.0017885614799385086, 'epoch': 1.42}\n",
            "{'loss': 0.5106, 'learning_rate': 0.0017878729733700302, 'epoch': 1.42}\n",
            "{'loss': 0.4606, 'learning_rate': 0.00178718348060905, 'epoch': 1.42}\n",
            "{'loss': 0.6321, 'learning_rate': 0.0017864930025186167, 'epoch': 1.42}\n",
            "{'loss': 0.5012, 'learning_rate': 0.0017858015399630119, 'epoch': 1.42}\n",
            "{'loss': 0.5071, 'learning_rate': 0.00178510909380775, 'epoch': 1.43}\n",
            "{'loss': 0.5007, 'learning_rate': 0.0017844156649195757, 'epoch': 1.43}\n",
            "{'loss': 0.5018, 'learning_rate': 0.001783721254166465, 'epoch': 1.43}\n",
            "{'loss': 0.6152, 'learning_rate': 0.0017830258624176223, 'epoch': 1.43}\n",
            "{'loss': 0.4917, 'learning_rate': 0.0017823294905434798, 'epoch': 1.43}\n",
            "{'loss': 0.5317, 'learning_rate': 0.0017816321394156964, 'epoch': 1.43}\n",
            "{'loss': 0.5496, 'learning_rate': 0.0017809338099071578, 'epoch': 1.43}\n",
            "{'loss': 0.5701, 'learning_rate': 0.0017802345028919725, 'epoch': 1.43}\n",
            "{'loss': 0.5388, 'learning_rate': 0.001779534219245475, 'epoch': 1.43}\n",
            " 72% 2009/2808 [4:39:29<6:07:05, 27.57s/it]Traceback (most recent call last):\n",
            "  File \"/content/LLaVA/llava/train/train_mem.py\", line 13, in <module>\n",
            "    train()\n",
            "  File \"/content/LLaVA/llava/train/train.py\", line 907, in train\n",
            "    trainer.train(resume_from_checkpoint=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2654, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2679, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 581, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 569, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/LLaVA/llava/model/language_model/llava_llama.py\", line 78, in forward\n",
            "    outputs = self.model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 685, in forward\n",
            "    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
            "    return CheckpointFunction.apply(function, preserve, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 506, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
            "    outputs = run_function(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 681, in custom_forward\n",
            "    return module(*inputs, output_attentions, None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 408, in forward\n",
            "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/LLaVA/llava/train/llama_flash_attn_monkey_patch.py\", line 88, in forward\n",
            "    x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flash_attn/bert_padding.py\", line 108, in unpad_input\n",
            "    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate ███████████▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss ▅▄▅▂▄▄▂▂▄▅▄▅▄▁▄▄▄▅▄▃▅▇▅▄▃▁▆▄▅▆▄▆▃▅▆▁▄▅█▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch 1.43\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step 2009\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate 0.00178\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.5388\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfast-dragon-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/checkday0837/huggingface/runs/5eykdizh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/checkday0837/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg4MjM2MTE1/version_details/v0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230806_112915-5eykdizh/logs\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#train\n",
        "!python /content/LLaVA/llava/train/train_mem.py \\\n",
        "    --model_name_or_path /content/LLaVA/vicuna-7b-v1.3 \\\n",
        "    --version v1 \\\n",
        "    --data_path /content/output.json \\\n",
        "    --image_folder /content/train \\\n",
        "    --vision_tower openai/clip-vit-large-patch14 \\\n",
        "    --tune_mm_mlp_adapter True \\\n",
        "    --mm_vision_select_layer -2 \\\n",
        "    --mm_use_im_start_end \\\n",
        "    --bf16 True \\\n",
        "    --output_dir \"/content/drive/MyDrive/ATL/LLaVA checkpoint\" \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --evaluation_strategy \"no\" \\\n",
        "    --save_strategy \"steps\" \\\n",
        "    --save_steps 250 \\\n",
        "    --save_total_limit 1 \\\n",
        "    --learning_rate 2e-3 \\\n",
        "    --weight_decay 0. \\\n",
        "    --warmup_ratio 0 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --logging_steps 1 \\\n",
        "    --tf32 True \\\n",
        "    --model_max_length 128 \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --lazy_preprocess True \\\n",
        "    --report_to wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdvu81MI_1P9",
        "outputId": "40c91f9d-d762-4e2f-c0ae-d08bc8da40f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-08-06 16:13:17,948] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-08-06 16:13:19.859337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading LLaVA from base model...\n",
            "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
            "/content/drive/MyDrive/ATL/월간 데이콘 이미지 기반 질의 응답 AI 경진대회/LLaVA checkpoint/LLaVA-7B-v1.3\n",
            "lmsys/vicuna-7b-v1.3\n",
            "Loading checkpoint shards: 100% 2/2 [00:11<00:00,  5.83s/it]\n",
            "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at lmsys/vicuna-7b-v1.3 and are newly initialized: ['model.mm_projector.weight', 'model.mm_projector.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0% 0/40479 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "100% 40479/40479 [1:23:57<00:00,  8.03it/s]\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/LLaVA/llava/eval/model_vqa.py \\\n",
        "    --model-path \"/content/drive/MyDrive/LLaVA checkpoint/LLaVA-7B-v1.3\"\\\n",
        "    --model-base lmsys/vicuna-7b-v1.3\\\n",
        "    --question-file \\\n",
        "    /content/test.jsonl \\\n",
        "    --image-folder \\\n",
        "   /content/test \\\n",
        "    --answers-file \\\n",
        "    /content/result.jsonl"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
